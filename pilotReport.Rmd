---
title: "COD Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

#### Article ID: UlhiU
#### Pilot: Kyle MacDonald
#### Co-pilot: Tom Hardwicke  
#### Start date: 4/4/17
#### End date: 4/10/17

-------

#### Methods summary: 

In this study, Wang & Busmeyer test whether generating a category decision will interfere with subsequent decisions about actions to take. On each trial, participants saw a face and were asked to either make a categorization (good face vs. bad face) *and* an action (attack vs. withdraw) decision(C-D trials), or to only make a categorization decision (C-alone) or an action decision (D-alone). Category membership was associated probabilistically with the width of the face, and, depending on the category of the face, participants received different rewards for taking different actions (e.g., good faces had a 70% chance to be rewarded for a withdraw action and 30% for an attack action; and bad faces had a 30% chance and a 70% chance). 

The dependent variables were the mean response probabilities in the different trial types and a derived measure of "interference" computed by taking the difference between the probabilities of action decision when it was made alone versus when it was made following a categorization event (p(A) - p(A|categorization). The key statistical tests were two one-sample t-tests testing the null hypothesis of no interference effect for the good faces and bad faces, and several correlations of interest.  

------

#### Target outcomes:

We will attempt to reproduce:

  1. the probabilities in the second two rows of the following table (Table 1 in the paper).

![](figs/table1.png)

  2. the two, separate one-sample t-tests, testing the interfence effect for type b faces (t(168)=2.24,SE=.015,p=.027) and type g faces (t(168)=.61,SE=.013,p=.54) against a null model of no intereference effect.
  
  3. the following correlations:
    - between p(G) and p′(G) (r=.52,p<.0001 for type b faces, r=.65,p<.0001 for type g faces); 
    - between p(A) and pT(A) (r=.46,p<.0001 for type b faces, r=.51,p<.0001 for type g faces). 
    - between the interference effects produced by the two different types of faces (r=-.16,p=.04). 
    
  4. the difference between p(A|B) for the two types of faces

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
library(magrittr) 
library(stringr)
```

## Step 2: Load data

Here's the data description provided by Wang & Busemeyer for Experiment 1 in their codebook. 

> Data are included in the file titled “Exp1(N=169).xlsx,” and each raw summarizes a participant’s behavioral responses. The first five columns summarize responses to the “good guy” type of face (i.e., type g faces). The second five columns summarize responses to the “bad guy” type of face (i.e., type b faces). Within each five columns, the first four columns are the frequencies of response combinations in the categorization-decision (C-D) trials. The fifth column is the frequency of “withdrawing” in the decision-alone (D-alone) trials. 

Skip reading in the first row since it encodes information about face type and not the variable names. 

```{r}
d <- readxl::read_excel("data/data.xlsx", skip = 1)
```

## Step 3: Tidy data

Add a participant ID column, so we don't lose this information when we convert from wide to long format in Step 3. Each row is data for an individual participant.

```{r}
d$id <- 1:nrow(d)
```

Split the dataset using column position, so I can add the information about face type as a variable. Columns 1-5 are type g faces and columns 6-10 are type b faces. 

```{r}
# split data
d.typeg <- bind_cols(d[1:5], d['id'])
d.typeb <- bind_cols(d[6:10], d['id'])

# add the face type 
d.typeg %<>% mutate(face_type = "type_g")
d.typeb %<>% mutate(face_type = "type_b")  

# put the data frames back together in "longer" format
d.tidy <- bind_rows(d.typeb, d.typeg)
```

Add the frequency of attack responses for the D-alone trials. This information was not included in the dataset, but we know there were 17 test trials for each face type for each participant. From the results section 3.2, 

> The estimated choice probabilities (i.e., sample proportions) were obtained for each participant and each type of face from the last block of C-D trials and from the transfer tests (D-alone trials, C-alone trials). Each estimate of a marginal probability is based on 17 choice trials per participant and each type of face: (p. 137)

So to get the frequency of attack responses, subtract the frequency of withdrawing responses from 17. 

```{r}
n_test_trials_per_face <- 17

d.tidy %<>% mutate(`Attack (D-alone)` = n_test_trials_per_face - `Friendly (D-alone)`)
```

Convert from wide format to long format using the gather() function.

```{r}
d.tidy.l <- d.tidy %>% 
  gather(key = trial_info, value = freq_response, 
         `Good&Friendly`:`Friendly (D-alone)`, `Attack (D-alone)`) 
```

Add information about C-D vs. D-alone trials. This is also specified by column position in the original data: 1-4 and 6-9 are C-D trials; 5 and 10 are D-alone trials. I am going to take advantage of the fact that the character "&" only occurs in the C-D variable names

```{r}
d.tidy.l %<>% mutate(trial_type = ifelse(str_detect(trial_info, pattern = "&"), 
                                            "C-D", "D-alone"))
```

Clean up the trial name variable to remove spaces and special characters.

```{r}
d.tidy.l %<>% 
  mutate(trial_info = gsub(.$trial_info, pattern = "&", replacement = "_")) %>% 
  mutate(trial_info = gsub(.$trial_info, pattern = " \\(D-alone\\)", replacement = ""))
```

Separate the information in the `trial_info` variable since this contains information about both the categorization (good or bad guy) and the decision to attack or withdraw.

```{r}
d.tidy.l %<>% 
  separate(col = trial_info, sep = "_", into = c("cat_decision", "attack_decision"),
           fill = "left") 
```

Change one of the levels of the attack decision variable from "Friendly" to "Withdraw" to be more consistent with the terminology used in the paper text. I clean up the category decision variable to be "none" for the D-alone trials where participants did not make a categorization response prior to an attack/withdraw decision. And I add the number of test trials for each 

```{r}
d.tidy.l %<>% 
  mutate(attack_decision = ifelse(attack_decision == "Friendly", "Withdraw", attack_decision),
         cat_decision = ifelse(is.na(cat_decision), "none", cat_decision))
```

### Some tests to check our wrangling 

Test to make sure we still have 169 unique participants after data munging.

```{r}
n.tidy <- d.tidy.l %>% 
  distinct(id) %>% 
  nrow()

n.tidy == nrow(d)
```

How many responses do we have for each participant? We don't have data for the C-alone trials, so my best guess is that we should expect 68 total responses for each participant (2 face types X 2 blocks X 17 trials in each block). 

```{r}
resp_check <- d.tidy.l %>% 
  group_by(id) %>% 
  summarise(n_resp = sum(freq_response)) %>% 
  .$n_resp == 68 

sum(resp_check) == 169
```

Each participant has 68 total responses. 

Ok, we now have tidy data in long format with the correct number of participants and trials. But before we can start running the stats, we need to remove the "optimizers" from the data set. From Wang & Busmeyer, 

> Some participants, whom we call "optimizers," always chose the "optimal" category for a particular type of face on C-D trials: 43 did so for the narrow faces and 31 did so for the wide faces (approximately 25% and 18%, respectively, of the 169 participants). These participants obey the law of total probability for either type of face for trivial reasons, and for these participants, we cannot estimate the conditional probabilities for non-chosen categories and thus cannot really estimate the total probability for an action decision. (p. 137)

I had to search earlier in the paper to find a definition for optimal behavior. 

> The optimal model describes the optimal behaviors. According to the optimal model, the decision to attack should depend only on the face. If a type b face is presented, then it is always optimal to attack, and if a type g face is presented, then it is always optimal to withdraw. (p. 134) 

It would have been helpful if Wang & Busmeyer had flagged these participants in the data. 

```{r}
# get the sample proportions 
d.tidy.l %<>% mutate(samp_prop = freq_response / n_test_trials_per_face)

# flag participants with 1.0 sample proportion for attacking type b faces and withdrawing on type g faces
d.tidy.l %<>%
  mutate(optimizer = ifelse((face_type == "type_b" & attack_decision == "Attack" & samp_prop == 1) |
                              (face_type == "type_g" & attack_decision == "Withdraw" & samp_prop == 1),
                            "optimizer", "non-optimizer"))
```

Make exclusions table to show how many participants were optimizers for each type of face

```{r}
d.tidy.l %>% 
  filter(optimizer == "optimizer") %>% 
  group_by(face_type) %>% 
  summarise(n_excluded = n()) %>% 
  kable()
```

My first attempt at reproducing their filtering procedure did not work. 

```{r}
# type g optimizers
compareValues(reportedValue = 43, obtainedValue = 12)
# type b optimizers
compareValues(reportedValue = 31, obtainedValue = 12)
```

```{r}
d.analysis <- d.tidy.l
d.filtered <- d.tidy.l %>% filter(optimizer == "non-optimizer")
```

Note that I chose to do the rest of the analysis on the unfiltered data because I was unable to reproduce their filtering criterion. Also, in the text, they say that:

> The statistical tests were computed using all 169 participants. (p. 138)

## Step 4: Run analysis

Some text about how to compute the marginal probabilties: 

![](figs/marg_prob_text.png)

There was also some important information about the computation buried in the caption of Table 3: 

> The empirical results shown in this table were obtained by first obtaining estimates for each individual, and then averaging the estimates across all participants.

### Descriptive statistics

Store some global variables common across all marginal probabilites 

```{r}
n_ss <- 169
n_cd_trials <- 34
n_d_alone_trials <- 34
n_trials_per_face_type <- n_cd_trials / 2
```

INSUFFICIENT INFORMATION ERROR

We can't reproduce $p'(G)$ or $p'(B)$, which is the probability of categorizing a face as good or bad on the categorization only trials (C-alone) because they did not provide these data.

Get $p(G)$ and $p(B)$, which is equal to the probability of categorizing a face as good or bad on C-D trials.

```{r}
ss_pg_pb <- d.analysis %>% 
  filter(trial_type == "C-D") %>% 
  group_by(id, face_type, cat_decision) %>% 
  summarise(freq = sum(freq_response),
            p_ss = freq / n_trials_per_face_type) 

ms_p_g_b <- ss_pg_pb %>% 
  group_by(face_type, cat_decision) %>% 
  summarise(p = mean(p_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 

ms_p_g_b %<>% 
  spread(key = cat_decision, value = p) %>% 
  rename(p_b = Bad, p_g = Good)
```

Next, let's try to reproduce the conditional probability $p(A|G)$ or the probability that the participants attacked given categorizing a face as good.

```{r}
ss_p_ag <- d.analysis %>% 
  filter(trial_type == "C-D", cat_decision == "Good") %>%
  group_by(id, face_type) %>% 
  mutate(n_trials = sum(freq_response),
         p_ss = ifelse(n_trials == 0, 0, freq_response / n_trials)) %>% # deal with the divide by zero 
  filter(attack_decision == "Attack")

ms_p_ag <- ss_p_ag %>% 
  group_by(face_type, attack_decision) %>% 
  summarise(p_a_given_g = mean(p_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) %>% 
  filter(attack_decision == "Attack") %>% 
  select(face_type, p_a_given_g)
```

Now, $p(A|B)$ or the probability that the participants attacked given categorizing a face as bad.

```{r}
ss_p_ab <- d.analysis %>% 
  filter(trial_type == "C-D", cat_decision == "Bad") %>%
  group_by(id, face_type) %>% 
  mutate(n_trials = sum(freq_response),
         p_ss = ifelse(n_trials == 0, 0, freq_response / n_trials)) %>% # deal with the divide by zero 
  filter(attack_decision == "Attack")

ms_p_ab <- ss_p_ab %>% 
  group_by(face_type, attack_decision) %>% 
  summarise(p_a_given_b = mean(p_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) %>% 
  filter(attack_decision == "Attack") %>% 
  select(face_type, p_a_given_b)
```

$p(A)$ or the proportion of attack choices on D-alone trials.

```{r}
ss_p_a <- d.analysis %>% 
  filter(trial_type == "D-alone", attack_decision == "Attack") %>% 
  group_by(id, face_type) %>% 
  mutate(freq_attack = sum(freq_response),
         p_attack_ss = freq_attack / n_trials_per_face_type)

ms_p_a <- ss_p_a %>% 
  group_by(face_type) %>% 
  summarise(p_attack = mean(p_attack_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 
```

$p_t(A)$ is the total proportion of attack choices across all C-D trials (combining the proportions through the two category selection paths)

```{r}
ss_pt_a <- d.analysis %>% 
  filter(trial_type == "C-D", attack_decision == "Attack") %>% 
  group_by(id, face_type) %>% 
  mutate(freq_attack = sum(freq_response),
         pt_attack_ss = freq_attack / n_trials_per_face_type) %>% 
  select(id, face_type, pt_attack_ss) %>% 
  distinct(.keep_all = T)
  
ms_pt_a <- ss_pt_a %>%   
  group_by(face_type) %>% 
  summarise(pt_attack = mean(pt_attack_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 
```

The interference effect, or the difference between attack decisions with and without a categorization decision: $interference = p(A) - p_t(A)$

```{r}
ss_int <- ss_pt_a %>% 
  select(id, pt_attack_ss, face_type) %>% 
  left_join(., select(ss_p_a, id, p_attack_ss, face_type)) %>% 
  distinct(., .keep_all = T) %>% 
  mutate(int_effect_ss = p_attack_ss - pt_attack_ss)

ms_int <- ss_int %>% 
  group_by(face_type) %>% 
  summarise(int = mean(int_effect_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 
```

Try to reproduce Table 1.

```{r}
ms_p_g_b %>% 
  mutate(n = NA,p_prime_g = NA) %>%
  left_join(., ms_p_ag) %>% 
  left_join(., ms_p_ab) %>% 
  left_join(., ms_p_a) %>% 
  left_join(., ms_pt_a) %>% 
  left_join(., ms_int) %>% 
  kable()
```

### Inferential statistics

First, the two, separate one-sample t-tests, testing the interfence effect for type b faces (t(168)=2.24,SE=.015,p=.027) and type g faces (t(168)=.61,SE=.013,p=.54) against a null model of no interference effect.

```{r}
ss_int %>% 
  filter(face_type == "type_b") %>% 
  .$int_effect_ss %>% 
  t.test(., mu = 0)
```

```{r}
ss_int %>% 
  filter(face_type == "type_g") %>% 
  .$int_effect_ss %>% 
  t.test(., mu = 0)
```

INSUFFICIENT INFORMATION ERROR

We can't compute the correlations between $p(G)$ and $p′(G)$ (r=.52,p<.0001 for type b faces, r=.65,p<.0001 for type g faces) because calculating $p′(G)$ requires the C-alone data, which we don't have.

```{r}

```

Correlation between p(A) and pT(A) for type b faces (r=.46,p<.0001);

```{r}
p_a_b <- ss_p_a %>% filter(face_type == "type_b") %>% .$p_attack_ss
pt_a_b <- ss_pt_a  %>% filter(face_type == "type_b") %>% .$pt_attack_ss
cor.test(p_a_b, pt_a_b)
```

Correlation between p(A) and pT(A) for type g faces (r=.51,p<.0001 for type g faces). 

```{r}
p_a_g <- ss_p_a %>% filter(face_type == "type_g") %>% .$p_attack_ss
pt_a_g <- ss_pt_a  %>% filter(face_type == "type_g") %>% .$pt_attack_ss
cor.test(p_a_g, pt_a_g)
```

Correlation between the interference effects produced by the two different types of faces (r=-.16,p=.04). 

```{r}
int_type_b <- ss_int %>% filter(face_type == "type_b") %>% .$int_effect_ss
int_type_g <- ss_int %>% filter(face_type == "type_g") %>% .$int_effect_ss
cor.test(int_type_b, int_type_g)
```

## Step 5: Conclusion

```{r}
codReport(Report_Type = 'pilot',
          Article_ID = 'UlhiU', 
          Insufficient_Information_Errors = 2,
          Decision_Errors = 0, 
          Major_Numerical_Errors = 2, 
          Minor_Numerical_Errors = 0)
```

In summary, the outcome of this report is a failure since I was not able to reproduce the probabilities in Table 2. The two main issues were: (1) as far as I could tell, the authors did not include the C-alone trials in the published data and (2) I was not able to reproduce the number of participants removed for the two face types using their filtering criterion (and these participants were not flagged in their data file). However, it's possible that I was not implementing the criterion correctly. In contrast to the descriptive statistics, I was able to reproduce the key inferential statistics from the paper. 

There were also several important pieces of information that were not clearly described in the results section: (1) information about needing to compute probabilities for each participant before averaging across participants was located in the caption of Table 3 and (2) information about the "optimizer" filtering criterion was in the model description section of the introduction (2.2.1. Optimal model). 

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
