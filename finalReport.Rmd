---
title: "COD Reproducibility Report"
output:
html_document:
toc: true
toc_float: true
---

#### Article ID: UlhiU
#### Pilot: Kyle MacDonald
#### Start date: Apr 4 2017
#### End date: Nov 21 2017
#### CoPilot: Mike Frank
#### Final verification: Tom Hardwicke
#### Date: Nov 21 2017

-------

#### Methods summary: 

In this study, Wang & Busmeyer test whether generating a category decision will interfere with subsequent decisions about actions to take. On each trial, participants saw a face and were asked to either make a categorization (good face vs. bad face) *and* an action (attack vs. withdraw) decision (C-D trials), or to only make a categorization decision (C-alone) or an action decision (D-alone). Category membership was associated probabilistically with the width of the face, and, depending on the category of the face, participants received different rewards for taking different actions (e.g., good faces had a 70% chance to be rewarded for a withdraw action and 30% for an attack action; and bad faces had a 30% chance and a 70% chance). 

The dependent variables were the mean response probabilities in the different trial types and a derived measure of "interference" computed by taking the difference between the probabilities of action decision when it was made alone versus when it was made following a categorization event: $p(A) - p(A|categorization)$. The key statistical tests were two one-sample t-tests testing the null hypothesis of no interference effect for the good faces and bad faces, and several correlations of interest.  

------

#### Target outcomes:

We will attempt to reproduce:

1. the probabilities in the second two rows of the following table (Table 1 in the paper).

![](figs/table1.png)

2. the two, separate one-sample $t$-tests, testing the interfence effect for type $b$ faces ($t(168)=2.24$, $SE=.015$, $p=.027$) and type $g$ faces ($t(168)=.61$, $SE=.013$, $p=.54$) against a null model of no interference effect.

3. the following correlations:
- between $p(G)$ and $p′(G)$ ($r=.52$, $p<.0001$ for type $b$ faces, $r=.65$, $p<.0001$ for type $g$ faces); 
- between $p(A)$ and $p_T(A)$ ($r=.46$, $p<.0001$ for type $b$ faces, $r=.51$, $p<.0001$ for type $g$ faces);
- between the interference effects produced by the two different types of faces ($r=-.16$, $p =.04$). 

4. the difference between $p(A|B)$ for the two types of faces

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)

# prepare an empty report object, we will update this each time we run compareValues2()
reportObject <- data.frame("Article_ID" = NA, "valuesChecked" = 0, "eyeballs" = 0, "Total_df" = 0, "Total_p" = 0, "Total_mean" = 0, "Total_sd" = 0, "Total_se" = 0, "Total_ci" = 0, "Total_bf" = 0, "Total_t" = 0, "Total_F" = 0, "Total_es" = 0, "Total_median" = 0, "Total_irr" = 0, "Total_r" = 0, "Total_z" = 0, "Total_coeff" = 0, "Total_n" = 0, "Total_x2" = 0, "Total_other" = 0, "Insufficient_Information_Errors" = 0, "Decision_Errors" = 0, "Major_Numerical_Errors" = 0, "Minor_Numerical_Errors" = 0, "Major_df" = 0, "Major_p" = 0, "Major_mean" = 0, "Major_sd" = 0, "Major_se" = 0, "Major_ci" = 0, "Major_bf" = 0, "Major_t" = 0, "Major_F" = 0, "Major_es" = 0, "Major_median" = 0, "Major_irr" = 0, "Major_r" = 0, "Major_z" = 0, "Major_coeff" = 0, "Major_n" = 0, "Major_x2" = 0, "Major_other" = 0, "affectsConclusion" = NA, "error_typo" = 0, "error_specification" = 0, "error_analysis" = 0, "error_data" = 0, "error_unidentified" = 0, "Author_Assistance" = NA, "resolved_typo" = 0, "resolved_specification" = 0, "resolved_analysis" = 0, "resolved_data" = 0, "correctionSuggested" = NA, "correctionPublished" = NA)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
library(magrittr) # for data munging
library(stringr) # for string manipulation
library(plotrix) # for std.error
```

## Step 2: Load data

Here's the data description provided by Wang & Busemeyer for Experiment 1 in their codebook. 

> Data are included in the file titled “Exp1(N=169).xlsx,” and each raw summarizes a participant’s behavioral responses. The first five columns summarize responses to the “good guy” type of face (i.e., type g faces). The second five columns summarize responses to the “bad guy” type of face (i.e., type b faces). Within each five columns, the first four columns are the frequencies of response combinations in the categorization-decision (C-D) trials. The fifth column is the frequency of “withdrawing” in the decision-alone (D-alone) trials. 

Skip reading in the first row since it encodes information about face type and not the variable names. 

```{r, eval = F}
d <- readxl::read_excel("data/data.xlsx", skip = 1)
```

NB: The initial data was missing C-alone trials so here we reproduce the analyses using two additional data files provided by the authors after asking for their assistance. 

```{r}
d_good <- read.table("authorAssistance/GFq.txt")
colnames(d_good) <- c("Good&Friendly", "Good&Attack",	"Bad&Friendly",	
                      "Bad&Attack",	"Friendly (D-alone)", "Friendly (C-alone)")

# bad face types
d_bad <- read.table("authorAssistance/BFq.txt")
colnames(d_bad) <- c("Good&Friendly", "Good&Attack", "Bad&Friendly",	
                     "Bad&Attack",	"Friendly (D-alone)", "Friendly (C-alone)")
```

## Step 3: Tidy data

Add a participant ID column, so we don't lose this information when we convert from wide to long format in Step 3. Each row is data for an individual participant. We also add the information about face type as a variable.

```{r}
# add the face type 
d_good %<>% mutate(face_type = "type_g",
                   id = 1:n())
d_bad %<>% mutate(face_type = "type_b",
                  id = 1:n())  

# put the data frames back together in "longer" format
d_tidy <- bind_rows(d_good, d_bad)
```

Add the frequency of attack responses for the D-alone trials. This information was not included in the dataset, but we know there were 17 test trials for each face type for each participant. From the results section 3.2, 

> The estimated choice probabilities (i.e., sample proportions) were obtained for each participant and each type of face from the last block of C-D trials and from the transfer tests (D-alone trials, C-alone trials). Each estimate of a marginal probability is based on 17 choice trials per participant and each type of face: (p. 137)

So to get the frequency of attack responses, subtract the frequency of withdrawing responses from 17. 

```{r}
n_test_trials_per_face <- 17

d_tidy %<>% 
  mutate(`Attack (D-alone)` = n_test_trials_per_face - `Friendly (D-alone)`,
         `Attack (C-alone)` = n_test_trials_per_face - `Friendly (C-alone)`)
```

Convert from wide format to long format using the `tidyr::gather` function.

```{r}
d_tidy_l <- d_tidy %>% 
  gather(key = trial_info, 
         value = freq_response, 
         `Good&Friendly`:`Friendly (C-alone)`, `Attack (D-alone)`:`Attack (C-alone)`) 
```

Add information about C-D vs. D-alone trials. This is also specified by column position in the original data: 1-4 and 6-9 are C-D trials; 5 and 10 are D-alone trials. I am going to take advantage of the fact that the character `&` only occurs in the C-D variable names

```{r}
d_tidy_l %<>% 
  mutate(trial_type = ifelse(str_detect(trial_info, 
                                        pattern = "&"), "C-D", 
                             ifelse(str_detect(trial_info, pattern = "D-alone"), "D-alone",
                                    "C-alone")))
```

Clean up the trial name variable to remove spaces and special characters.

```{r}
d_tidy_l %<>% 
  mutate(trial_info = gsub(.$trial_info, 
                           pattern = "&", 
                           replacement = "_")) %>% 
  mutate(trial_info = gsub(.$trial_info, 
                           pattern = " \\(D-alone\\)", 
                           replacement = "")) %>% 
   mutate(trial_info = gsub(.$trial_info, 
                           pattern = " \\(C-alone\\)", 
                           replacement = ""))
```

Separate the information in the `trial_info` variable since this contains information about both the categorization (good or bad guy) and the decision to attack or withdraw.

```{r}
d_tidy_l %<>% 
  separate(col = trial_info, 
           sep = "_", 
           into = c("cat_decision", "attack_decision"),
           fill = "left") 
```

Change one of the levels of the attack decision variable from "Friendly" to "Withdraw" to be more consistent with the terminology used in the paper text. I clean up the category decision variable to be "none" for the D-alone trials where participants did not make a categorization response prior to an attack/withdraw decision. 

```{r}
d_tidy_l %<>% 
  mutate(attack_decision = ifelse(attack_decision == "Friendly", 
                                  "Withdraw", attack_decision),
         cat_decision = ifelse(is.na(cat_decision), 
                               "none", cat_decision))
```

### Some tests to check our wrangling 

Test to make sure we still have 169 unique participants after data munging.

```{r}
n_tidy <- d_tidy_l %>% 
  distinct(id) %>% 
  nrow()

n_tidy == 169
```

How many responses do we have for each participant? We don't have data for the C-alone trials, so my best guess is that we should expect 68 total responses for each participant (2 face types X 2 blocks X 17 trials in each block). 

NB: After author assistance, we now have the C-alone trials. So we should expect 102 trials (2 face types X 3 blocks X 17 trials in each block).

```{r}
resp_check <- d_tidy_l %>% 
  group_by(id) %>% 
  summarise(n_resp = sum(freq_response)) %>% 
  .$n_resp == 102 

sum(resp_check) == 169
```

Each participant has 102 total responses. Thus, we now have tidy data in long format with the correct number of participants and trials. 

Are all participants represented in the data? From Wang & Busmeyer, 

> Some participants, whom we call "optimizers," always chose the "optimal" category for a particular type of face on C-D trials: 43 did so for the narrow faces and 31 did so for the wide faces (approximately 25% and 18%, respectively, of the 169 participants). These participants obey the law of total probability for either type of face for trivial reasons, and for these participants, we cannot estimate the conditional probabilities for non-chosen categories and thus cannot really estimate the total probability for an action decision. (p. 137)

But we believe it will not be possible to reproduce this filtering criterion given the reported data. We do not have data tabulated for wide/narrow faces, only for good and bad faces (which is only probabilistically related to the face type). 

Nevertheless, considering the `N` column in Table 1 reveals a coincidence, namely that $N_bad = 126 = 169 - 43$ and $N_good = 138 = 169 - 31$. It is unclear to us how this filtering procedure was done, because these $N$s should only match if "narrow face optimizers" were removed for "bad guy" trials, and the text reports that

> Participants were randomly assigned to one of the associations between face features and types of faces: (1) the narrow faces with thick lips were more likely to be bad guys (the type b faces) or (2) they were more likely to be good guys (the type g faces). (p. 137)

In any case, in the text, it's stated that:

> The statistical tests were computed using all 169 participants. (p. 138)

So we keep the full, unfiltered dataset for computing the inferential stats later on in the analysis.

```{r}
d_analysis_unfilt <- d_tidy_l
```

NB: After author assistance, we can better implement their filtering criteria. Specifically, they say: 

> We only included participants that made at least one incorrect categorization of a type of face. That is: Only Ps that categorized Good at least once when face was b = bad guy type face. Only Ps that categorized Bad at least once when face was g = good guy type face.

> As said above, 169 saw bad faces, 43 of these always categorized this as bad (zero good face categories when it was a b = bad type face); 169 saw good faces, 31 of these always categorized this as a good face (zero bad guy face categories when it was a g = good guy type face).

So we filter out participants who did not make any incorrect classifications for either the good or bad face types. We expect to get: $N_{bad} = 126 = 169 - 43$ and $N_{good} = 138 = 169 - 31$

```{r}
d_filtered <- d_tidy_l %>%
  filter(trial_type == "C-D") %>% 
  group_by(id, face_type, cat_decision) %>% 
  summarise(freq_response = sum(freq_response)) %>% 
  mutate(optimizer = ifelse(freq_response == 0 | freq_response == 17, 
                            "optimizer", "non_optimizer"))

d_filtered %<>% 
  select(id, face_type, optimizer) %>% 
  unique() 

n <- d_filtered %>% 
  group_by(face_type, optimizer) %>% 
  count()

n %>% kable()
```

We are able to reproduce their filtering criteria and can now try to reproduce the descriptive stats.

```{r}
d_analysis <- left_join(d_tidy_l, d_filtered, by = c("id", "face_type"))

d_analysis %<>% filter(optimizer == "non_optimizer")
```

Check to make sure we have the correct number of participants.

```{r}
d_analysis %>% 
  select(id, face_type, optimizer) %>% 
  unique() %>% 
  group_by(face_type, optimizer) %>% 
  count()
```

We have the correct number of participants.

## Step 4: Run analysis

Some text about how to compute the marginal probabilties: 

![](figs/marg_prob_text.png)

There was also some important information about the computation in the caption of Table 3: 

> The empirical results shown in this table were obtained by first obtaining estimates for each individual, and then averaging the estimates across all participants.

### Descriptive statistics

Store some global variables common across all marginal probabilites 

```{r}
n_ss <- 169
n_cd_trials <- 34
n_d_alone_trials <- 34
n_c_alone_trials <- 34
n_trials_per_face_type <- n_cd_trials / 2
```

NB: After the authors sent us the data for the C-alone trials, we can now reproduce $p'(G)$ and $p'(B)$.

```{r}
ss_pgp_pbp <- d_analysis %>% 
  filter(trial_type == "C-alone") %>% 
  group_by(id, face_type, attack_decision) %>% 
  summarise(freq = sum(freq_response),
            p_ss = freq / n_trials_per_face_type) 

ms_p_g_b_p <- ss_pgp_pbp %>% 
  group_by(face_type, attack_decision) %>% 
  summarise(p = mean(p_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 

ms_p_g_b_p %<>% 
  spread(key = attack_decision, value = p) %>% 
  rename(p_b_prime = Attack, p_g_prime = Withdraw)
```

Get $p(G)$ and $p(B)$, which is equal to the probability of categorizing a face as good or bad on C-D trials.

```{r}
ss_pg_pb <- d_analysis %>% 
  filter(trial_type == "C-D") %>% 
  group_by(id, face_type, cat_decision) %>% 
  summarise(freq = sum(freq_response),
            p_ss = freq / n_trials_per_face_type) 

ms_p_g_b <- ss_pg_pb %>% 
  group_by(face_type, cat_decision) %>% 
  summarise(p = mean(p_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 

ms_p_g_b %<>% 
  spread(key = cat_decision, value = p) %>% 
  rename(p_b = Bad, p_g = Good)
```

Next, let's try to reproduce the conditional probability $p(A|G)$ or the probability that the participants attacked given categorizing a face as good.

```{r}
ss_p_ag <- d_analysis %>% 
  filter(trial_type == "C-D", cat_decision == "Good") %>%
  group_by(id, face_type) %>% 
  mutate(n_trials = sum(freq_response),
         p_ss = ifelse(n_trials == 0, 0, freq_response / n_trials)) %>% # deal with the divide by zero 
  filter(attack_decision == "Attack")

ms_p_ag <- ss_p_ag %>% 
  group_by(face_type, attack_decision) %>% 
  summarise(p_a_given_g = mean(p_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) %>% 
  filter(attack_decision == "Attack") %>% 
  select(face_type, p_a_given_g)
```

Now, $p(A|B)$ or the probability that the participants attacked given categorizing a face as bad.

```{r}
ss_p_ab <- d_analysis %>% 
  filter(trial_type == "C-D", cat_decision == "Bad") %>%
  group_by(id, face_type) %>% 
  mutate(n_trials = sum(freq_response),
         p_ss = ifelse(n_trials == 0, 0, freq_response / n_trials)) %>% # deal with the divide by zero 
  filter(attack_decision == "Attack")

ms_p_ab <- ss_p_ab %>% 
  group_by(face_type, attack_decision) %>% 
  summarise(p_a_given_b = mean(p_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) %>% 
  filter(attack_decision == "Attack") %>% 
  select(face_type, p_a_given_b)
```

$p(A)$ or the proportion of attack choices on D-alone trials.

```{r}
ss_p_a <- d_analysis %>% 
  filter(trial_type == "D-alone", attack_decision == "Attack") %>% 
  group_by(id, face_type) %>% 
  mutate(freq_attack = sum(freq_response),
         p_attack_ss = freq_attack / n_trials_per_face_type)

ms_p_a <- ss_p_a %>% 
  group_by(face_type) %>% 
  summarise(p_attack = mean(p_attack_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 
```

$p_t(A)$ is the total proportion of attack choices across all C-D trials (combining the proportions through the two category selection paths)

```{r}
ss_pt_a <- d_analysis %>% 
  filter(trial_type == "C-D", attack_decision == "Attack") %>% 
  group_by(id, face_type) %>% 
  mutate(freq_attack = sum(freq_response),
         pt_attack_ss = freq_attack / n_trials_per_face_type) %>% 
  select(id, face_type, pt_attack_ss) %>% 
  distinct(.keep_all = T)

ms_pt_a <- ss_pt_a %>%   
  group_by(face_type) %>% 
  summarise(pt_attack = mean(pt_attack_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 
```

The interference effect, or the difference between attack decisions with and without a categorization decision: $interference = p(A) - p_t(A)$

```{r}
ss_int <- ss_pt_a %>% 
  select(id, pt_attack_ss, face_type) %>% 
  left_join(., select(ss_p_a, id, p_attack_ss, face_type)) %>% 
  distinct(., .keep_all = T) %>% 
  mutate(int_effect_ss = p_attack_ss - pt_attack_ss)

ms_int <- ss_int %>% 
  group_by(face_type) %>% 
  summarise(int = mean(int_effect_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 
```

Try to reproduce the key rows of Table 1:

![](figs/table1_key.png)

```{r}
sumdat <- ms_p_g_b_p %>% 
  left_join(., filter(n, optimizer == "non_optimizer")) %>% 
  left_join(., ms_p_g_b) %>% 
  left_join(., ms_p_ag) %>% 
  left_join(., ms_p_ab) %>% 
  left_join(., ms_p_a) %>% 
  left_join(., ms_pt_a) %>% 
  left_join(., ms_int) %>% 
  select(face_type, n, p_g_prime, p_g, p_a_given_g, p_b, p_a_given_b, pt_attack, p_attack, int) 

sumdat %>%
  kable(digits = 2)
```

Check values explicitly:

```{r}
# type b face
n <- sumdat %>% filter(face_type == 'type_b') %>% pull(n)
reportObject <- compareValues2(reportedValue = "126", obtainedValue = n, valueType = 'n')

pg1 <- sumdat %>% filter(face_type == 'type_b') %>% pull(p_g_prime)
reportObject <- compareValues2(reportedValue = ".20", obtainedValue = pg1, valueType = 'n')

pg2 <- sumdat %>% filter(face_type == 'type_b') %>% pull(p_g)
reportObject <- compareValues2(reportedValue = ".21", obtainedValue = pg2, valueType = 'n')

pag <- sumdat %>% filter(face_type == 'type_b') %>% pull(p_a_given_g)
reportObject <- compareValues2(reportedValue = ".41", obtainedValue = pag, valueType = 'n')

pb <- sumdat %>% filter(face_type == 'type_b') %>% pull(p_b)
reportObject <- compareValues2(reportedValue = ".79", obtainedValue = pb, valueType = 'n')

pab <- sumdat %>% filter(face_type == 'type_b') %>% pull(p_a_given_b)
reportObject <- compareValues2(reportedValue = ".58", obtainedValue = pab, valueType = 'n')

pra <- sumdat %>% filter(face_type == 'type_b') %>% pull(pt_attack)
reportObject <- compareValues2(reportedValue = ".55", obtainedValue = pra, valueType = 'n')

pa <- sumdat %>% filter(face_type == 'type_b') %>% pull(p_attack)
reportObject <- compareValues2(reportedValue = ".59", obtainedValue = pa, valueType = 'n')

int <- sumdat %>% filter(face_type == 'type_b') %>% pull(int)
reportObject <- compareValues2(reportedValue = ".04", obtainedValue = int, valueType = 'n')

# type g face
n <- sumdat %>% filter(face_type == 'type_g') %>% pull(n)
reportObject <- compareValues2(reportedValue = "138", obtainedValue = n, valueType = 'n')

pg1 <- sumdat %>% filter(face_type == 'type_g') %>% pull(p_g_prime)
reportObject <- compareValues2(reportedValue = ".79", obtainedValue = pg1, valueType = 'n')

pg2 <- sumdat %>% filter(face_type == 'type_g') %>% pull(p_g)
reportObject <- compareValues2(reportedValue = ".78", obtainedValue = pg2, valueType = 'n')

pag <- sumdat %>% filter(face_type == 'type_g') %>% pull(p_a_given_g)
reportObject <- compareValues2(reportedValue = ".39", obtainedValue = pag, valueType = 'n')

pb <- sumdat %>% filter(face_type == 'type_g') %>% pull(p_b)
reportObject <- compareValues2(reportedValue = ".22", obtainedValue = pb, valueType = 'n')

pab <- sumdat %>% filter(face_type == 'type_g') %>% pull(p_a_given_b)
reportObject <- compareValues2(reportedValue = ".52", obtainedValue = pab, valueType = 'n')

pra <- sumdat %>% filter(face_type == 'type_g') %>% pull(pt_attack)
reportObject <- compareValues2(reportedValue = ".41", obtainedValue = pra, valueType = 'n')

pa <- sumdat %>% filter(face_type == 'type_g') %>% pull(p_attack)
reportObject <- compareValues2(reportedValue = ".42", obtainedValue = pa, valueType = 'n')

int <- sumdat %>% filter(face_type == 'type_g') %>% pull(int)
reportObject <- compareValues2(reportedValue = ".01", obtainedValue = int, valueType = 'n')
```

NB: After author assistance, we can now reproduce all of the descriptive statistics in rows 3 and 4 of Table 1. 

### Inferential statistics

Note that the authors did not filter participants for any of the inferential tests, so we need to re-compute all of the probabilities using the unfiltered data. 

```{r}
######### ss_pg_prime
ss_pgp_pbp <- d_analysis_unfilt %>% 
  filter(trial_type == "C-alone") %>% 
  group_by(id, face_type, attack_decision) %>% 
  summarise(freq = sum(freq_response),
            p_ss = freq / n_trials_per_face_type) 

ms_p_g_b_p <- ss_pgp_pbp %>% 
  group_by(face_type, attack_decision) %>% 
  summarise(p = mean(p_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 

ms_p_g_b_p %<>% 
  spread(key = attack_decision, value = p) %>% 
  rename(p_b_prime = Attack, p_g_prime = Withdraw)


######### ss_pg_pb
ss_pg_pb <- d_analysis_unfilt %>% 
  filter(trial_type == "C-D") %>% 
  group_by(id, face_type, cat_decision) %>% 
  summarise(freq = sum(freq_response),
            p_ss = freq / n_trials_per_face_type) 

ms_p_g_b <- ss_pg_pb %>% 
  group_by(face_type, cat_decision) %>% 
  summarise(p = mean(p_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 

ms_p_g_b %<>% 
  spread(key = cat_decision, value = p) %>% 
  rename(p_b = Bad, p_g = Good)

######### ss_p_ab
ss_p_ab <- d_analysis_unfilt %>% 
  filter(trial_type == "C-D", cat_decision == "Bad") %>%
  group_by(id, face_type) %>% 
  mutate(n_trials = sum(freq_response),
         p_ss = ifelse(n_trials == 0, 0, freq_response / n_trials)) %>% # deal with the divide by zero 
  filter(attack_decision == "Attack")

ms_p_ab <- ss_p_ab %>% 
  group_by(face_type, attack_decision) %>% 
  summarise(p_a_given_b = mean(p_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) %>% 
  filter(attack_decision == "Attack") %>% 
  select(face_type, p_a_given_b)

######### ss_pa
ss_p_a <- d_analysis_unfilt %>% 
  filter(trial_type == "D-alone", attack_decision == "Attack") %>% 
  group_by(id, face_type) %>% 
  mutate(freq_attack = sum(freq_response),
         p_attack_ss = freq_attack / n_trials_per_face_type)

ms_p_a <- ss_p_a %>% 
  group_by(face_type) %>% 
  summarise(p_attack = mean(p_attack_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 

######### ss_pt_a
ss_pt_a <- d_analysis_unfilt %>% 
  filter(trial_type == "C-D", attack_decision == "Attack") %>% 
  group_by(id, face_type) %>% 
  mutate(freq_attack = sum(freq_response),
         pt_attack_ss = freq_attack / n_trials_per_face_type) %>% 
  select(id, face_type, pt_attack_ss) %>% 
  distinct(.keep_all = T)

ms_pt_a <- ss_pt_a %>%   
  group_by(face_type) %>% 
  summarise(pt_attack = mean(pt_attack_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 

######### ss_int
ss_int <- ss_pt_a %>% 
  select(id, pt_attack_ss, face_type) %>% 
  left_join(., select(ss_p_a, id, p_attack_ss, face_type)) %>% 
  distinct(., .keep_all = T) %>% 
  mutate(int_effect_ss = p_attack_ss - pt_attack_ss)

ms_int <- ss_int %>% 
  group_by(face_type) %>% 
  summarise(int = mean(int_effect_ss)) %>% 
  mutate_if(is.numeric, round, digits = 2) 
```

Now we can do the statistical tests.

First, the two, separate one-sample t-tests, testing the interfence effect for type $b$ faces ($t(168)=2.24$, $SE=.015$, $p=.027$) and type $g$ faces ($t(168)=.61$, $SE=.013$, $p=.54$) against a null model of no interference effect.

```{r}
b_t_test <- ss_int %>% 
  filter(face_type == "type_b") %>% 
  .$int_effect_ss %>% 
  t.test(., mu = 0)
b_t_test

se <- ss_int %>% 
  filter(face_type == "type_b") %>% 
  .$int_effect_ss %>% 
  std.error()
```

```{r}
reportObject <- compareValues2(reportedValue = "168", obtainedValue = b_t_test$parameter, valueType = 'df')
reportObject <- compareValues2(reportedValue = "2.24", obtainedValue = b_t_test$statistic, valueType = 't')
reportObject <- compareValues2(reportedValue = ".027", obtainedValue = b_t_test$p.value, valueType = 'p')
reportObject <- compareValues2(reportedValue = ".015", obtainedValue = se, valueType = 'se')
```

```{r}
g_t_test <- ss_int %>% 
  filter(face_type == "type_g") %>% 
  .$int_effect_ss %>% 
  t.test(., mu = 0)

g_t_test

se <- ss_int %>% 
  filter(face_type == "type_g") %>% 
  .$int_effect_ss %>% 
  std.error()
```

```{r}
reportObject <- compareValues2(reportedValue = "168", obtainedValue = g_t_test$parameter, valueType = 'df')
reportObject <- compareValues2(reportedValue = ".61", obtainedValue = g_t_test$statistic, valueType = 't')
reportObject <- compareValues2(reportedValue = ".54", obtainedValue = g_t_test$p.value, valueType = 'p')
reportObject <- compareValues2(reportedValue = ".013", obtainedValue = se, valueType = 'se')
```

NB: After author assistance, we can try to compute the correlations between $p(G)$ and $p′(G)$ ($r=.52$, $p<.0001$) or for type $b$ faces, $r=.65$, $p<.0001$ for type $g$ faces)

```{r}
p_g_b <- ss_pg_pb %>% 
  filter(face_type == "type_b", cat_decision == "Good") %>% 
  pull(p_ss)

pg_prime_b <- ss_pgp_pbp %>% 
  filter(face_type == "type_b", attack_decision == "Withdraw") %>%
  pull(p_ss)

cor_pg_pgprime_b <- cor.test(p_g_b, pg_prime_b)
cor_pg_pgprime_b
```

```{r}
reportObject <- compareValues2(reportedValue = ".52", obtainedValue = cor_pg_pgprime_b$estimate, valueType = 'r')
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = cor_pg_pgprime_b$p.value, valueType = 'p')
```

Do the same thing but for the type $g$ faces.

```{r}
p_g_g <- ss_pg_pb %>% 
  filter(face_type == "type_g", cat_decision == "Good") %>% 
  pull(p_ss)

pg_prime_g <- ss_pgp_pbp %>% 
  filter(face_type == "type_g", attack_decision == "Withdraw") %>%
  pull(p_ss)

cor_pg_pgprime_g <- cor.test(p_g_g, pg_prime_g)
cor_pg_pgprime_g
```

```{r}
reportObject <- compareValues2(reportedValue = ".65", obtainedValue = cor_pg_pgprime_g$estimate, valueType = 'r')
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = cor_pg_pgprime_g$p.value, valueType = 'p')
```

Correlation between $p(A)$ and $p_T(A)$ for type $b$ faces ($r=.46$, $p<.0001$);

```{r}
# NB - this code relies on the descriptives being done without filtering, as is currently the case. 

p_a_b <- ss_p_a %>% filter(face_type == "type_b") %>% .$p_attack_ss
pt_a_b <- ss_pt_a  %>% filter(face_type == "type_b") %>% .$pt_attack_ss
cor_pa_pta_b <- cor.test(p_a_b, pt_a_b)
cor_pa_pta_b
```

```{r}
reportObject <- compareValues2(reportedValue = ".46", obtainedValue = cor_pa_pta_b$estimate, valueType = 'r')
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = cor_pa_pta_b$p.value, valueType = 'p')
```

Correlation between $p(A)$ and $p_T(A)$ for type $g$ faces ($r=.51$, $p<.0001$ for type $g$ faces). 

```{r}
p_a_g <- ss_p_a %>% filter(face_type == "type_g") %>% .$p_attack_ss
pt_a_g <- ss_pt_a  %>% filter(face_type == "type_g") %>% .$pt_attack_ss
cor_pa_pta_g <- cor.test(p_a_g, pt_a_g)
cor_pa_pta_g
```

```{r}
reportObject <- compareValues2(reportedValue = ".51", obtainedValue = cor_pa_pta_g$estimate, valueType = 'r')
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = cor_pa_pta_g$p.value, valueType = 'p')
```

Correlation between the interference effects produced by the two different types of faces ($r=-.16$, $p=.04$). 

```{r}
int_type_b <- ss_int %>% filter(face_type == "type_b") %>% .$int_effect_ss
int_type_g <- ss_int %>% filter(face_type == "type_g") %>% .$int_effect_ss
cor_int <- cor.test(int_type_b, int_type_g)
cor_int
```

```{r}
reportObject <- compareValues2(reportedValue = "-.16", obtainedValue = cor_int$estimate, valueType = 'r')
reportObject <- compareValues2(reportedValue = "0.04", obtainedValue = cor_int$p.value, valueType = 'p')
```

## Step 5: Conclusion

In summary, the outcome of this report is a success with author assistance. Specifically, the authors provided: 

  1. the C-alone trials that were not present in the published data 
  2. more information about how to impement their filtering criterion (these participants were not flagged in their data file). 

With the authors' help, we were able to reproduce all of the descriptive statistics and the key inferential statistics from the paper, with only a couple of minor numerical errors. 

```{r}
reportObject$Article_ID <- "UlhiU"
reportObject$affectsConclusion <- NA
reportObject$error_typo <- 0
reportObject$error_specification <- 0
reportObject$error_analysis <- 0
reportObject$error_data <- 0
reportObject$error_unidentified <- 0
reportObject$Author_Assistance <- T
reportObject$resolved_typo <- 0
reportObject$resolved_specification <- 1
reportObject$resolved_analysis <- 0
reportObject$resolved_data <- 1
reportObject$correctionSuggested <- "no"
reportObject$correctionPublished <- F

# decide on final outcome
if(reportObject$Decision_Errors > 0 | reportObject$Major_Numerical_Errors > 0 | reportObject$Insufficient_Information_Errors > 0){
  reportObject$finalOutcome <- "Failure"
  if(reportObject$Author_Assistance == T){
    reportObject$finalOutcome <- "Failure despite author assistance"
  }
}else{
  reportObject$finalOutcome <- "Success"
  if(reportObject$Author_Assistance == T){
    reportObject$finalOutcome <- "Success with author assistance"
  }
}

# save the report object
filename <- paste0("reportObject_", reportObject$Article_ID,".csv")
write_csv(reportObject, filename)
```

## Report Object

```{r, echo = FALSE}
# display report object in chunks
kable(reportObject[2:10], align = 'l')
kable(reportObject[11:20], align = 'l')
kable(reportObject[21:25], align = 'l')
kable(reportObject[26:30], align = 'l')
kable(reportObject[31:35], align = 'l')
kable(reportObject[36:40], align = 'l')
kable(reportObject[41:45], align = 'l')
kable(reportObject[46:51], align = 'l')
kable(reportObject[52:57], align = 'l')
```

## Session information
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```

